

！！！！！最关键的操作！！！如果使用ceph功能，就要关闭RAID功能！！！因为功能冲突，会导致效率急剧下降！！！！！！！！！！！！


先确认一下ceph存储服务是ok的：ceph -s
	确认三个mon都是启动的，HEALTH_OK或者HEALTH_WARN状态 都是可以的，WARN的原因可能是因为时间不同步，如果是ERROR就是错误
服务没有启动的原因：
	我们用chown给 共享磁盘 改了所属主所属组都是ceph，这样，客户端才能访问共享磁盘，而chown是临时改权限，想要永久更改权限，
  要用到之前的udev功能，来实现开机自动改权限， 当然我们也可以给共享磁盘设置acl权限

以昨天的实验环境为基础：
	4台主机，3台mon+osd
	ceph软件
	hosts解析
	key无密码连接服务器
	NTP时间服务器

———————————————————————————————————————————————————————————————————————————————————————————————

基于快照生成一个共享磁盘，这个镜像盘里可就是有数据的非空盘：
1.先把快照保护起来，因为如果有人把快照删了，那么基于这张快照生成的盘就不能用了：
	rbd snap protect image --snap image-snap1  //image镜像里有个image-snap1
	一旦保护起来之后，想要用命令删除快照就会失败 
	rbd snap rm image --snap image-snap1    //会失败
2.保护起来之后就可以基于快照克隆了：
	rbd clone image --snap image-snap1 image-clone --image-feature layering
	//使用image的快照image-snap1克隆一个新的叫image-clone的镜像，它的功能是layering
	克隆完了，客户端就可以mount挂载使用了
3.查看克隆镜像与父镜像快照的关系：
	rbd info image-clone 显示内容如下：
    	rbd image 'image-clone':
    	size 15360 MB in 3840 objects
    	order 22 (4096 kB objects)
   	block_name_prefix: rbd_data.d3f53d1b58ba
    	format: 2
    	features: layering
    	flags: 
    	parent: rbd/image@image-snap1
	#克隆镜像很多数据都来自于快照链
4.如果希望克隆镜像可以独立工作，就需要将父快照中的数据，全部拷贝一份，但比较耗时！
	rbd flatten image-clone #拷贝完之后，那个快照就没有用了
	你再查，就不会有parent关系了
补充知识点：
	1.当你把6个osd里其中的某个osd关掉了，ceph集群会自动recovery进行数据还原，自动remap迁移数据，这个执行状态可以用ceph -s命令查看
	2.有时候你用systemctl启动一个服务的时候，如果出错了，它不会指出有用的信息，你要是用绝对路径的那种启动方式，就会显示有用的信息，
  	而以绝对路径启动服务，需要用 （ ps aux | grep 服务名 ） 这条命令去找程序的绝对路径
5.客户端撤销磁盘映射：
	umount /mnt
	rbd showmapped
	rbd unmap /dev/rbd0
	删除快照与镜像：
	rbd snap rm image --snap image-snap
	rbd  list //查看信息
	rbd  rm  image

——————————————————————————————————————————————————————————————————————————————————————————————

关于虚拟化：单台机器叫虚拟机，组成集群了就叫虚拟化
虚拟机的组成：镜像文件【也就是虚拟机的磁盘，真机里的一个文件】+配置文件
	关于虚拟机镜像文件：
		真机里：cd /var/lib/libvirt/images/虚拟机名称.qcow
		新建一个虚拟机，虚拟机不是有磁盘吗，这个磁盘其实是真机上的一个文件，你给虚拟机装的系统、软件、所有的东西！其实都装到了真机里的这个文件下面了
		你ls -ld看这个文件，它显示的是虚拟机磁盘最大空间的大小，你用du -h命令可以查看这个文件，目前所存储的数据到底是多少。
	关于虚拟机配置文件：
		真机里：cd /etc/libvirt/qemu/虚拟机名称.xml
		这个文件里放的是虚拟机里所有的硬件配置！什么cpu 显卡 内存 磁盘 等等硬件信息都在这个文件里！
		让我们来看看这个文件：
	底下这段话是描述磁盘的，有多个磁盘的话，就是有多对<disk>,每多一个磁盘，就会多一个镜像文件
	<disk type='file' device='disk'>    //disk这是磁盘，磁盘的类型是真机里的一个文件
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/rh7_node02.img'/>
      <target dev='vda' bus='virtio'/>
      <boot order='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
	</disk>
	底下这段话，写的是网卡信息<interface>
	<interface type='network'>
      <mac address='52:54:00:d7:15:e4'/>
      <source network='private2'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x09' function='0x0'/>
      </interface>

只要拷贝了镜像文件和配置文件，这俩文件，虚拟机就出来了，所以我们以后也可以快照这两个文件，然后这两个文件就相当于是base虚拟机了，以后就不用它们了，只用快照的虚拟机
当然快照的原理也说过了，是相当于给原文件做了个快捷方式，但是你改快捷方式里的内容不会影响原文件，但是原文件是会影响快照的。
所以如果别人想拷贝你的虚拟机，你给他快照是不可以的，因为快照是快捷方式啊。

现在来做一下这个利用快照做新虚拟机的实验：
	1.给虚拟机镜像文件，和配置文件做快照，镜像文件，和配置文件的模版在：
	/var/lib/libvirt/images目录下有模版
	/var/lib/libvirt/images/.rh7_template.img 	#镜像文件模版
	/var/lib/libvirt/images/.rhel7.xml 		#配置文件模版
	基于镜像文件模版创建一个快照的命令：
		qemu-img create -f qcow2 -b /var/lib/libvirt/images/.rh7_template.img new.qcow2 10G
		#我要创建一个快照，是一个文件，它的格式是qcow2，我是基于/var/lib/libvirt/images/.rh7_template.img这个文件做的快照，快照名叫new.qcow2，这个文件大小为10G		
		这个文件，就是新虚拟机的镜像文件，这个新建的文件是一个快照，所以大小才几百K
	2.现在要拷贝模版配置文件，并做一些修改：
	cp /var/lib/libvirt/images/.rhel7.xml /etc/libvirt/qemu/new.xml
	 vim /etc/libvirt/qemu/new.xml
	改名字
		<name>名字</name>
	改磁盘
		<disk>
		<source file='/var/lib/libvirt/images/new.qcow2'>    #因为你要建的虚拟机，磁盘不是那个快照文件吗，所以要改这行
		</disk> 			
	3.这个配置文件改完了，以后就用这个配置文件做新虚拟机的配置文件，还有刚才快照的镜像文件，现在该用这俩文件创建新虚拟机了：
		virsh define /var/lib/libvirt/images/new.xml
		#利用配置文件创建虚拟机，硬件都是什么要求都写在这个配置文件里了，所以，这一条命令就OK了！	
	以上就搞定这个新虚拟机了！

这个虚拟机和ceph的关联：
	虚拟机的磁盘不是真实机里的一个文件吗，但是当我真实机如果坏了怎么办！所以就要靠ceph了！让虚拟机的镜像文件，调用的是ceph的image镜像，也就是共享磁盘空间当虚拟机的磁盘！
	ceph自己的健康检查，可以几秒钟，就解决这个问题！
	要想实现这个功能，只要我们在/etc/libvirt/qemu/new.xml配置文件里，指定磁盘的source 路径为ceph的image镜像就可以了
再部署一遍ceph环境，之前的ceph如果保留了，就别重复做了：
1）创建磁盘镜像。
[root@node1 ~]# rbd create vm1-image --image-feature  layering --size 10G
[root@node1 ~]# rbd create vm2-image --image-feature  layering --size 10G
2）Ceph认证账户。
Ceph默认开启用户认证，客户端需要账户才可以访问，
默认账户名称为client.admin，key是账户的密钥，
可以使用ceph auth添加新账户（案例我们使用默认账户）。
 	cat /etc/ceph/ceph.conf          //配置文件 
	[global]
	mon_initial_members = node1, node2, node3
	mon_host = 192.168.2.10,192.168.2.20,192.168.2.30
	auth_cluster_required = cephx                                   //开启认证
	auth_service_required = cephx                           //开启认证
	auth_client_required = cephx                             //开启认证
	[root@node1 ~]# cat /etc/ceph/ceph.client.admin.keyring    //账户文件
	[client.admin]
    	key = AQBTsdRapUxBKRAANXtteNUyoEmQHveb75bISg==
3）用客户端来验证这个虚拟机调用ceph磁盘空间的效果，因为，你虚拟机就是在真机里启动的
部署客户端环境。
注意：这里使用真实机当客户端！！！
客户端需要安装ceph-common软件包，拷贝配置文件（否则不知道集群在哪），
拷贝连接密钥（否则无连接权限）。
用客户端来验证这个虚拟机调用ceph磁盘空间的效果，因为，你虚拟机就是在真机里启动的
[root@room9pc01 ~]# yum -y  install ceph-common
[root@room9pc01 ~]# scp 192.168.4.11:/etc/ceph/ceph.conf  /etc/ceph/
[root@room9pc01 ~]# scp 192.168.4.11:/etc/ceph/ceph.client.admin.keyring \
/etc/ceph/       #把这俩文件拷贝真机底下，这些实验，和昨天的都是一样的！



4）配置libvirt secret（用户名密码）  我们虚拟要告知机ceph服务的共享账户名UUID，和ceph服务的密码，将UUID和密码存放到虚拟环境里做成一个文件，以后虚拟机想要用ceph服务做磁盘的话，就调用这个文件！
我们先要创建出一个用户名的UUID：先根据ceph的用户名写一个文件，这个操作是在真机上操作
	[root@room9pc01 ~]# vim secret.xml            //这个文件是没有的，要自己写！且里面的内容要原封不动的写！ 
		<secret ephemeral='no' private='no'>
        	<usage type='ceph'>
                <name>client.admin secret</name>  #这个用户名是，ceph指定的用户名是client.admin 后面写secret是固定格式
        	</usage>
		</secret>	
然后我们要利用命令，将这个文件宣告到虚拟环境里，并自动用里面的账户名生成UUID：
	[root@room9pc01 ~]# virsh secret-define --file secret.xml   #我们要基于secret.xml这个文件生成一个账户及UUID号，并告知给虚拟环境
	733f0fd1-e3d6-4c25-a69f-6681fc19802b  			    #这个就是随机生成的用户UUID     

	[root@room9pc01 ~]# cat /etc/ceph/ceph.client.admin.keyring #来看一下密码是什么！这是真机用来访问ceph集群存储时需要的密码！所以虚拟机也用这个密码！

然后再通过命令，来告知整个虚拟环境访问ceph用什么UUID，以及使用这个UUID对应的密码是什么！
	[root@room9pc01] virsh secret-set-value \
	--secret  733f0fd1-e3d6-4c25-a69f-6681fc19802b\    #这里secret后面写的是之前创建的secret的UUID
	--base64 abcdef123456789 				 #这里写的是查到的密码
	
	现在虚拟环境中既有账户信息又有密钥信息，只需要在虚拟机的配置文件中告知UUID，就可以访问Ceph服务了。


6）虚拟机的XML配置文件。
每个虚拟机都会有一个XML配置文件，包括：虚拟机的名称、内存、CPU、磁盘、网卡等信息    
现在我们要修改我们上午创建的虚拟机的配置文件，使其指定ceph的共享盘做为虚拟机的磁盘，并告诉虚拟机用什么UUID访问ceph
[root@room9pc01 ~]# vim /etc/libvirt/qemu/vm1.xml  也可以用 virsh edit vm1命令 意思是我要编辑我的虚拟机vm1，且更改完立刻生效！
//修改前内容如下
	<disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/vm1.qcow2'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    	</disk>
	修改后内容如下：
	<disk type='network' device='disk'>  #network类型是网盘
      <driver name='qemu' type='raw'/>   #raw意思是裸盘！
      <auth username='admin'>  	#这个admin是固定格式，是指定访问ceph服务，用什么UUID的部分！    
      <secret type='ceph' uuid='......'/>  #填写利用virsh secret-list命令查看到的UUID，也就是用secret.xml生成的
      </auth>
      <source protocol='rbd' name='rbd/...'>   #protocol=用的是rbd共享池，name=用的盘是rbd/下什么名的盘 
	    <host name='192.168.4.11' port='6789'/>     </source>   #这个共享盘应该去找谁要，应该管那3个mon代理要，写它们仨谁的IP都行，当然写node1最好
      <target dev='vda' bus='virtio'/>  #dev=你希望这个盘到虚拟机里叫什么名 bus=你要用什么类型的接口做磁盘接口，7版本里的虚拟机类型要用virtio！真机是sata接口 
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>  #如果提示你什么0x07冲突，那就随便改下数
      </disk>

因为虚拟机是在真机上创建的，所以写虚拟机的配置文件，是直接写真机里的ceph image镜像的绝对路径
以后虚拟机所有的数据都是写在云盘里了
你的虚拟机配置文件，也是写在ceph提供的存储空间的，这个配置文件不用涉及到什么登陆密码什么的，我们是在块存储的时候写的

————————————————————————————————————————————————————————————————————————————————————


关于ceph存储支持：mon服务的端口号是6789
	块存储（客户多了块盘）
	文件系统存储，就像nfs ftp samba是的，客户只需要mount就可以用了，但是需要注意的是，ceph只能做一个目录共享，不能像其它服务那样做多个！

现在说一下文件系统：
	买回来的磁盘，你需要分区，格式化   格式完之后的分区就是一个文件系统
	而格式化真正做的是将这个分区再做规划，使其分区变成两大部分，一部分叫inode，一部分叫block，并且这两部分都是由很多个4K的小块空间组成的
	inode部分用于存储 你要存的数据的名称啊，权限啊，这个大的数据存放在block的哪些小块里，等等一些描述信息
	block部分是真正用于存你要保存的数据的，并且会将大的数据分成多份，每份4K，随机存放在不同小块空间里，等你想要用这些数据的时候，系统会去问inode这些数据都存在block的哪个小块里
	4K就是分配单元，是默认的，可以改，你改大了，如果要存数据是很小的，就会造成空间浪费，可能占不满一个4K的空间，如果你数据很大的话，存到好多个4K小块里，找起来就慢一些，所以看你自
  己的需求，当然一般没人改这玩意，因为你可能存大数据也可能存小数据
	当你删除一个文件时，实际上是给inode部分里这个打一个标记，告诉系统这个标记的数据可以删！真正的存储数据还在block区域里，这些block里的数据，就变成可用空间了，你只要再往磁盘里写
  数据，很有可能就会直接覆盖掉，那就真的不能找回来这些数据了，想要去掉inode那个可删标记，可以用特殊软件easyrecovery之类的数据还原软件
	block因为是用来存真正数据的，所以空间要比inode部分大很多

用Ceph做文件系统的共享：我们给ceph的空间做完文件系统，客户挂载直接拿过去就能用！
	用ceph的osd空间做文件系统共享，可以用node1 2 3提供的osd空间，也可以新建虚拟机做
	如果想做文件系统的共享，需要启动一个服务MDS
	而osd服务，不就相当于提供了一块没有文件系统的裸盘吗，可以用来做文件系统的block部分，而启一个MDS服务，是提供另一块空间，用来做文件系统的inode部分
下面来做具体的实验：
	为了保证实验的易懂，还是新建一台虚拟机，单独配置，当然如果不新建的话，因为python的那个工具把所有的包都装了，会省去很多步骤
	1.添加一台新的虚拟机，要求如下：
		IP地址:192.168.4.14
		主机名:node4
		配置yum源（包括rhel、ceph的源）
		与Client主机同步时间
		node1允许无密码远程node4	
	2.部署元数据服务器：
		登陆node4，安装ceph-mds软件包
		[root@node4 ~]# yum -y install ceph-mds 
	3.登陆node1部署节点操作：
		[root@node1 ~]# cd  /root/ceph-cluster
		//该目录，是最早部署ceph集群时，创建的目录
		[root@node1 ceph-cluster]# ceph-deploy mds create node4
		//给nod4拷贝配置文件，启动mds服务
	4.同步配置文件和key:
		[root@node1 ceph-cluster]# ceph-deploy admin node4
	以上操作如果是在node1 2 3的其中一台做的话，那就直接启MDS服务就可以了：cd到/root/ceph-cluster再ceph-deploy mds create node3且不用做拷贝文件什么的操作了。
	5.现在要重新创建共享池：
		ceph osd pool create cephfs_data 128
		//创建存储池，对应128个PG，想写多少都可以，默认单位是PG，最好是2的平方数！当然你要是不确定填多少合适，可以去红帽官方找专门计算这个值的软件	
	 	ceph osd pool create cephfs_metadata 128
		//创建存储池，对应128个PG，想写多少都可以，默认单位是PG,这个PG是分组的意思，写多少都行
		这俩池子，一个用来存数据（block），一个用来存描述信息（inode）
	6.有了这两个池子之后就可以用ceph共享池创建文件系统了：
		ceph fs new myfs1 cephfs_metadata cephfs_data #先写metadata池做inode，再写data池做block，myfs1是文件系统名，叫啥都行
			new fs with metadata pool 2 and data pool 1 #这个是显示出来的信息不是命令
		在ceph里只能做一个文件系统！
		ceph fs ls 命令可以查看ceph的文件系统
	7.客户端挂载：
		mount -t ceph 192.168.4.11:6789:/  /mnt/cephfs/ -o name=admin,secret=AQBTsdRapUxBKRAANXtteNUyoEmQHveb75bISg==
		#-t：挂载ceph文件系统 是从mon节点服务器（192.168.4.11:6789:/ 根分区的意思）挂载来的 name=admin是用户名简称,secret是密钥，cat密钥文件找的密码，注意是临时挂载！
	客户端就多了个磁盘，这样，客户就不用给这块盘做文件系统了！这就叫做对象存储！拿过来就可以用！

——————————————————————————————————————————————————————————————————————————————————————————

ceph提供的存储方式：
块共享iscsi （客户端挂载加格式化）
文件系统共享nfs samba （客户端挂载即可使用）
对象存储共享swift  百度云盘  （客户端不用挂载不用格式化，而是通过API即一个APP访问） API需要开发人员编写程序
	下载专门的软件[云盘] web网站上传，来下载【代码，php程序，java程序等】
		
ceph-radosgw这个模块软件就是用来支持对象存储的软件包
radosgw这个单词是网关的意思

有个大前提，你玩这个服务的时候，要确定你整个ceph集群都是开的，别连osd mon服务都没起来！

现在来玩这个服务：
	再准备一台新的机器node5，让这台机器做rgw服务
	把准备工作做一下：
	1）准备实验环境，要求如下：
	IP地址:192.168.4.15
	主机名:node5
	配置yum源（包括rhel、ceph的源）
	与Client主机同步时间
	node1允许无密码远程node5
	修改node1的/etc/hosts，并同步到所有node主机
	[root@node1 ~]# ceph-deploy install --rgw node5 #用node1的deploy工具给node5装这个包，当然也可以手动去node5装

	还要给node5传密钥和配置文件：
	    	[root@node1 ~]# cd /root/ceph-cluster
   	    	[root@node1 ~]# ceph-deploy admin node5

	然后给node5启动服务：RGW服务的默认端口号是7480
		[root@node1 ~]# ceph-deploy rgw create node5 #这是用deploy自动启服务
		然后可以去node5里看，7480端口起来没：ps aux |grep 7480   客户访问这个端口就相当于云盘！
		为了让用户用起来更方便，修改为8000或80更方便客户端记忆和使用
		[root@node5 ~]#  vim  /etc/ceph/ceph.conf
			[client.rgw.node5]    //客户访问对象存储时，主机叫啥就叫啥
			host = node5
			rgw_frontends = "civetweb port=8000"
			//node5为主机名
			//civetweb是RGW内置的一个web服务
		如果改了端口，那就重启服务：[root@node5 ~]# systemctl  status ceph-radosgw@\*    #命令长，自己tab看服务名全称，这是手动起服务
		客户再访问192.168.4.15:8000  （4.15是mon的IP）就可以看到对象存储的页面了，但是这时候就是空页面，需要开发来写这个页面了




登陆node5（RGW）创建用于登陆云盘的账户和密码：
    [root@node5 ~]#  radosgw-admin user create --uid="testuser" --display-name="First User"
         			#这个命令用来自动生成云盘用户与密钥，用户名叫testuser，显示为First User
    … …
    "keys": [
            {
                "user": "testuser",
                "access_key": "5E42OEGB1M95Y49IBG7B",
                "secret_key": "i8YtM8cs7QDCK3rTRopb0TTPBFJVXdEryRbeLGK6"
            }
        ],
    ... ...
    #
    [root@node5 ~]# radosgw-admin user info --uid=testuser
    //查看testuser用户的信息，key是账户访问密钥

如果非要做这个云盘的测试想看效果就做以下操作
客户端安装软件（这个软件是第三方软件） #这个软件就是为了测试，企业内部不用这个，是企业开发人员写的图形化的APP

    [root@client ~]#  yum install s3cmd-2.0.1-1.el7.noarch.rpm   #这个是第三方软件，得去网上找包

客户端修改软件配置（注意，除了下面设置的内容，其他提示都默认回车）

    [root@client ~]#  s3cmd --configure
    Access Key: 5E42OEGB1M95Y49IBG7BSecret Key: i8YtM8cs7QDCK3rTRopb0TTPBFJVXdEryRbeLGK6
    S3 Endpoint [s3.amazonaws.com]: 192.168.4.15:8000
    [%(bucket)s.s3.amazonaws.com]: %(bucket)s.192.168.4.15:8000
    Use HTTPS protocol [Yes]: No
    Test access with supplied credentials? [Y/n] n
    Save settings? [y/N] y
    //注意，其他提示都默认回车

4）创建存储数据的bucket（类似于存储数据的目录）

    [root@client ~]# s3cmd ls
    [root@client ~]# s3cmd mb s3://my_bucket
    Bucket 's3://my_bucket/' created
    [root@client ~]# s3cmd ls
    2018-05-09 08:14 s3://my_bucket
    [root@client ~]# s3cmd put /var/log/messages s3://my_bucket/log/
    [root@client ~]# s3cmd ls
    2018-05-09 08:14 s3://my_bucket
    [root@client ~]# s3cmd ls s3://my_bucket
    DIR s3://my_bucket/log/
    [root@client ~]# s3cmd ls s3://my_bucket/log/
    2018-05-09 08:19 309034 s3://my_bucket/log/messages 

测试下载功能

    [root@client ~]# s3cmd get s3://my_bucket/log/messages /tmp/
/
测试删除功能

    [root@client ~]# s3cmd del s3://my_bucket/log/messages











	









